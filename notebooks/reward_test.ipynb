{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44610a09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.phyvpuzzle.agents.reward_agent import ActionCandidate, OpenAIRewardAgent, TransformersRewardAgent\n",
    "from src.phyvpuzzle.core.config import StepSelectionConfig\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e99f1cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a process supervision model for evaluating actions in physics-based tasks.\n",
      "\n",
      "## Task Description:\n",
      "Pick up the red cube and place it in the blue container\n",
      "\n",
      "## Current Object Information:\n",
      "{'red_cube': {'id': 1, 'position': [0.5, 0.3, 0.1], 'color': 'red', 'shape': 'cube'}, 'blue_container': {'id': 2, 'position': [0.8, 0.5, 0.0], 'color': 'blue', 'shape': 'container'}}\n",
      "\n",
      "## Previous Interaction History:\n",
      "['Step 1: Moved gripper to position [0.5, 0.3, 0.2]', 'Step 2: Opened gripper']\n",
      "\n",
      "## Proposed Action:\n",
      "- Action Type: grasp\n",
      "- Parameters: {\n",
      "  \"object_id\": 1,\n",
      "  \"position\": [\n",
      "    0.5,\n",
      "    0.3,\n",
      "    0.1\n",
      "  ],\n",
      "  \"force\": 0.5\n",
      "}\n",
      "- Agent's Reasoning: The red cube is within reach and the gripper is positioned correctly above it. Grasping it now will allow us to proceed with the task.\n",
      "\n",
      "## Your Task:\n",
      "Evaluate whether this action is correct and helpful for completing the task.\n",
      "\n",
      "Consider:\n",
      "1. Visual Accuracy: Are visual elements from the image correctly identified (shapes, colors, positions, quantities, spatial relationships)?\n",
      "2. Physical Validity: Does this action make physical sense given the current state?\n",
      "3. Task Progress: Does it move toward the task goal efficiently?\n",
      "4. Logical Reasoning: Is the agent's reasoning sound and logical?\n",
      "5. Risk Assessment: Are there any risks of making the task harder to complete?\n",
      "\n",
      "Response:\n",
      "• \"+\" if the action is correct and helpful for task completion\n",
      "• \"-\" if the action has any errors or is unhelpful\n",
      "\n",
      "Only respond with \"+\" or \"-\". No explanations.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 定义示例变量\n",
    "task_description = \"Pick up the red cube and place it in the blue container\"\n",
    "object_mapping = {\n",
    "    \"red_cube\": {\"id\": 1, \"position\": [0.5, 0.3, 0.1], \"color\": \"red\", \"shape\": \"cube\"},\n",
    "    \"blue_container\": {\"id\": 2, \"position\": [0.8, 0.5, 0.0], \"color\": \"blue\", \"shape\": \"container\"}\n",
    "}\n",
    "interaction_history = [\n",
    "    \"Step 1: Moved gripper to position [0.5, 0.3, 0.2]\",\n",
    "    \"Step 2: Opened gripper\"\n",
    "]\n",
    "\n",
    "# 定义候选动作对象\n",
    "class Candidate:\n",
    "    def __init__(self):\n",
    "        self.action_type = \"grasp\"\n",
    "        self.parameters = {\"object_id\": 1, \"position\": [0.5, 0.3, 0.1], \"force\": 0.5}\n",
    "        self.response = \"The red cube is within reach and the gripper is positioned correctly above it. Grasping it now will allow us to proceed with the task.\"\n",
    "\n",
    "candidate = Candidate()\n",
    "\n",
    "prompt = f\"\"\"You are a process supervision model for evaluating actions in physics-based tasks.\n",
    "\n",
    "## Task Description:\n",
    "{task_description}\n",
    "\n",
    "## Current Object Information:\n",
    "{object_mapping}\n",
    "\n",
    "## Previous Interaction History:\n",
    "{interaction_history}\n",
    "\n",
    "## Proposed Action:\n",
    "- Action Type: {candidate.action_type}\n",
    "- Parameters: {json.dumps(candidate.parameters, indent=2)}\n",
    "- Agent's Reasoning: {candidate.response}\n",
    "\n",
    "## Your Task:\n",
    "Evaluate whether this action is correct and helpful for completing the task.\n",
    "\n",
    "Consider:\n",
    "1. Visual Accuracy: Are visual elements from the image correctly identified (shapes, colors, positions, quantities, spatial relationships)?\n",
    "2. Physical Validity: Does this action make physical sense given the current state?\n",
    "3. Task Progress: Does it move toward the task goal efficiently?\n",
    "4. Logical Reasoning: Is the agent's reasoning sound and logical?\n",
    "5. Risk Assessment: Are there any risks of making the task harder to complete?\n",
    "\n",
    "Response:\n",
    "• \"+\" if the action is correct and helpful for task completion\n",
    "• \"-\" if the action has any errors or is unhelpful\n",
    "\n",
    "Only respond with \"+\" or \"-\". No explanations.\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ddaaaa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2 files: 100%|██████████| 2/2 [00:00<00:00, 34952.53it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型输出: +\n",
      "31.007083892822266 30.146175384521484\n",
      "tensor([0.7029, 0.2971])\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "import torch\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"ob11/Qwen-VL-PRM-3B\")\n",
    "model = AutoModelForImageTextToText.from_pretrained(\"ob11/Qwen-VL-PRM-3B\", device_map=\"cpu\")\n",
    "\n",
    "# 测试示例\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# 准备测试图像\n",
    "url = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# 准备对话\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            # {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "            {\"type\": \"text\", \"text\": prompt},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# 处理输入\n",
    "text_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "inputs = processor(text=text_prompt, images=[image], return_tensors=\"pt\").to(\"cpu\")\n",
    "\n",
    "# 生成输出\n",
    "output_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n",
    "output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "print(\"模型输出:\", output_text[0])\n",
    "\n",
    "\n",
    "from torch.nn import functional as F\n",
    "\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "\n",
    "# Get the logits for the next token position\n",
    "next_token_logits = logits[0, -1, :]\n",
    "\n",
    "# Get token IDs for \"+\" and \"-\"\n",
    "plus_token_id = processor.tokenizer.encode(\"+\", add_special_tokens=False)[0]\n",
    "minus_token_id = processor.tokenizer.encode(\"-\", add_special_tokens=False)[0]\n",
    "\n",
    "# Extract logits for + and - tokens\n",
    "plus_logit = next_token_logits[plus_token_id].item()\n",
    "minus_logit = next_token_logits[minus_token_id].item()\n",
    "\n",
    "print(plus_logit, minus_logit)\n",
    "\n",
    "# Apply softmax to get probabilities\n",
    "logits_tensor = torch.tensor([plus_logit, minus_logit])\n",
    "probs = F.softmax(logits_tensor, dim=0)\n",
    "\n",
    "print(probs)\n",
    "\n",
    "# Return probability of + token as reward score (0-1 range)\n",
    "# Convert to 1-5 scale for consistency\n",
    "prob_plus = probs[0].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db77339b",
   "metadata": {},
   "source": [
    "# Step Selection 方法对比示例\n",
    "\n",
    "本示例展示三种 Step Selection 方法：\n",
    "1. **Reward Model - Generative** (Transformers 本地模型)：1-5 分评分\n",
    "2. **Reward Model - Discriminative** (Transformers 本地模型)：+/- 二元评分\n",
    "3. **Pairwise Judge** (OpenAI GPT-5)：成对比较排序\n",
    "\n",
    "---\n",
    "\n",
    "## 方法 1: Reward Model - Generative (1-5 分)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a7f77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置 Reward Model - Generative 模式 (1-5 分)\n",
    "config_reward_generative = StepSelectionConfig(\n",
    "    enabled=True,\n",
    "    selection_method=\"reward_model\",\n",
    "    num_candidates=3,\n",
    "    top_k=3,\n",
    "    reward_model_type=\"transformers\",\n",
    "    reward_model_name=\"ob11/Qwen-VL-PRM-3B\",\n",
    "    reward_model_mode=\"generative\",  # 使用 generative 模式\n",
    "    reward_device=\"auto\",\n",
    "    reward_torch_dtype=\"auto\"\n",
    ")\n",
    "\n",
    "# 创建 Transformers Reward Model Agent (Generative)\n",
    "reward_agent_generative = TransformersRewardAgent(config_reward_generative)\n",
    "\n",
    "print(\"✓ Reward Model Agent - Generative (Qwen-VL-PRM-3B) 初始化完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363d7581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备测试数据\n",
    "# 创建一个简单的测试图像\n",
    "current_image = Image.new('RGB', (512, 512), color='white')\n",
    "\n",
    "# 任务描述\n",
    "task_description = \"\"\"\n",
    "3D CUBE STACKING PUZZLE\n",
    "\n",
    "You have 7 3D puzzle pieces and one container.\n",
    "\n",
    "TASK:\n",
    "Assemble all pieces into the container to form a solid 3×3×3 cube (27 unit cubes total).\n",
    "\n",
    "GOAL:\n",
    "- Fit every piece completely inside the container.\n",
    "- No gaps, overlaps, or floating pieces.\n",
    "- The final structure must be stable and form a perfect cube.\n",
    "\"\"\"\n",
    "\n",
    "# 对象映射\n",
    "object_mapping = \"\"\"\n",
    "OBJECT MAPPING (object_id → properties):\n",
    "============================================================\n",
    "object_id=2, RGB=(153, 48, 97)\n",
    "object_id=3, RGB=(169, 27, 0)\n",
    "object_id=4, RGB=(0, 42, 113)\n",
    "============================================================\n",
    "Total movable objects: 3\n",
    "\"\"\"\n",
    "\n",
    "# 交互历史\n",
    "interaction_history = \"Step 0: Initial state\"\n",
    "\n",
    "print(\"✓ 测试数据准备完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0a88a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建候选 actions（用于 Reward Model）\n",
    "candidates_reward = [\n",
    "    ActionCandidate(\n",
    "        action_type=\"move_object\",\n",
    "        parameters={\"object_id\": 2, \"target_position\": [0.1, 0.2, 0.3]},\n",
    "        response=\"I will move object 2 to position (0.1, 0.2, 0.3) to start building the structure.\",\n",
    "        tool_call={\"id\": \"call_1\", \"type\": \"function\", \"function\": {\"name\": \"move_object\", \"arguments\": '{\"object_id\": 2, \"target_position\": [0.1, 0.2, 0.3]}'}}\n",
    "    ),\n",
    "    ActionCandidate(\n",
    "        action_type=\"move_object\",\n",
    "        parameters={\"object_id\": 3, \"target_position\": [0.0, 0.0, 0.1]},\n",
    "        response=\"I will move object 3 to the base position to create a stable foundation.\",\n",
    "        tool_call={\"id\": \"call_2\", \"type\": \"function\", \"function\": {\"name\": \"move_object\", \"arguments\": '{\"object_id\": 3, \"target_position\": [0.0, 0.0, 0.1]}'}}\n",
    "    ),\n",
    "    ActionCandidate(\n",
    "        action_type=\"move_object\",\n",
    "        parameters={\"object_id\": 4, \"target_position\": [-0.1, 0.1, 0.2]},\n",
    "        response=\"I will place object 4 at this position to complete the first layer.\",\n",
    "        tool_call={\"id\": \"call_3\", \"type\": \"function\", \"function\": {\"name\": \"move_object\", \"arguments\": '{\"object_id\": 4, \"target_position\": [-0.1, 0.1, 0.2]}'}}\n",
    "    )\n",
    "]\n",
    "\n",
    "# 使用 Reward Model - Generative 评分\n",
    "print(\"使用 Reward Model - Generative (1-5 分) 方法评分...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "best_actions_generative = reward_agent_generative.select_best_actions(\n",
    "    candidates=candidates_reward,\n",
    "    current_image=current_image,\n",
    "    task_description=task_description,\n",
    "    object_mapping=object_mapping,\n",
    "    interaction_history=interaction_history\n",
    ")\n",
    "\n",
    "print(f\"\\nReward Model - Generative 选择了 top-{len(best_actions_generative)} 个最佳 actions:\\n\")\n",
    "\n",
    "for i, action in enumerate(best_actions_generative, 1):\n",
    "    print(f\"第 {i} 名:\")\n",
    "    print(f\"  Action: {action.action_type}\")\n",
    "    print(f\"  Parameters: {action.parameters}\")\n",
    "    print(f\"  Score: {action.score:.3f} (1-5 分)\")\n",
    "    print(f\"  Reasoning: {action.response[:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1117ad42",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 方法 2: Reward Model - Discriminative (+/- 二元)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41f356a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Reward Model Agent - Discriminative (Qwen-VL-PRM-3B) 初始化完成\n"
     ]
    }
   ],
   "source": [
    "# 配置 Reward Model - Discriminative 模式 (+/- 二元)\n",
    "config_reward_discriminative = StepSelectionConfig(\n",
    "    enabled=True,\n",
    "    selection_method=\"reward_model\",\n",
    "    num_candidates=3,\n",
    "    top_k=2,\n",
    "    reward_model_type=\"transformers\",\n",
    "    reward_model_name=\"ob11/Qwen-VL-PRM-3B\",\n",
    "    reward_model_mode=\"discriminative\",  # 使用 discriminative 模式\n",
    "    reward_device=\"auto\",\n",
    "    reward_torch_dtype=\"auto\"\n",
    ")\n",
    "\n",
    "# 创建 Transformers Reward Model Agent (Discriminative)\n",
    "reward_agent_discriminative = TransformersRewardAgent(config_reward_discriminative)\n",
    "\n",
    "print(\"✓ Reward Model Agent - Discriminative (Qwen-VL-PRM-3B) 初始化完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53365f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用 Reward Model - Discriminative (+/-) 方法评分...\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Fetching 2 files: 100%|██████████| 2/2 [00:00<00:00, 38304.15it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 35.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reward Model - Discriminative 选择了 top-3 个最佳 actions:\n",
      "\n",
      "第 1 名:\n",
      "  Action: move_object\n",
      "  Parameters: {'object_id': 2, 'target_position': [0.1, 0.2, 0.3]}\n",
      "  Score: 3.490 (基于 +/- logits 的概率)\n",
      "  Reasoning: I will move object 2 to position (0.1, 0.2, 0.3) to start building the structure....\n",
      "\n",
      "第 2 名:\n",
      "  Action: move_object\n",
      "  Parameters: {'object_id': 3, 'target_position': [0.0, 0.0, 0.1]}\n",
      "  Score: 3.490 (基于 +/- logits 的概率)\n",
      "  Reasoning: I will move object 3 to the base position to create a stable foundation....\n",
      "\n",
      "第 3 名:\n",
      "  Action: move_object\n",
      "  Parameters: {'object_id': 4, 'target_position': [-0.1, 0.1, 0.2]}\n",
      "  Score: 3.490 (基于 +/- logits 的概率)\n",
      "  Reasoning: I will place object 4 at this position to complete the first layer....\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 创建候选 actions（用于 Discriminative 模式）\n",
    "current_image = Image.new('RGB', (512, 512), color='white')\n",
    "candidates_discriminative = [\n",
    "    ActionCandidate(\n",
    "        action_type=\"move_object\",\n",
    "        parameters={\"object_id\": 2, \"target_position\": [0.1, 0.2, 0.3]},\n",
    "        response=\"I will move object 2 to position (0.1, 0.2, 0.3) to start building the structure.\",\n",
    "        tool_call={\"id\": \"call_1\", \"type\": \"function\", \"function\": {\"name\": \"move_object\", \"arguments\": '{\"object_id\": 2, \"target_position\": [0.1, 0.2, 0.3]}'}}\n",
    "    ),\n",
    "    ActionCandidate(\n",
    "        action_type=\"move_object\",\n",
    "        parameters={\"object_id\": 3, \"target_position\": [0.0, 0.0, 0.1]},\n",
    "        response=\"I will move object 3 to the base position to create a stable foundation.\",\n",
    "        tool_call={\"id\": \"call_2\", \"type\": \"function\", \"function\": {\"name\": \"move_object\", \"arguments\": '{\"object_id\": 3, \"target_position\": [0.0, 0.0, 0.1]}'}}\n",
    "    ),\n",
    "    ActionCandidate(\n",
    "        action_type=\"move_object\",\n",
    "        parameters={\"object_id\": 4, \"target_position\": [-0.1, 0.1, 0.2]},\n",
    "        response=\"I will place object 4 at this position to complete the first layer.\",\n",
    "        tool_call={\"id\": \"call_3\", \"type\": \"function\", \"function\": {\"name\": \"move_object\", \"arguments\": '{\"object_id\": 4, \"target_position\": [-0.1, 0.1, 0.2]}'}}\n",
    "    )\n",
    "]\n",
    "\n",
    "# 使用 Reward Model - Discriminative 评分\n",
    "print(\"使用 Reward Model - Discriminative (+/-) 方法评分...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "best_actions_discriminative = reward_agent_discriminative.select_best_actions(\n",
    "    candidates=candidates_discriminative,\n",
    "    current_image=current_image,\n",
    "    task_description=\"Build a stable structure using the available objects\",\n",
    "    object_mapping={2: \"red_cube\", 3: \"blue_cube\", 4: \"green_cube\"},\n",
    "    interaction_history=[]\n",
    ")\n",
    "\n",
    "print(f\"\\nReward Model - Discriminative 选择了 top-{len(best_actions_discriminative)} 个最佳 actions:\\n\")\n",
    "\n",
    "for i, action in enumerate(best_actions_discriminative, 1):\n",
    "    print(f\"第 {i} 名:\")\n",
    "    print(f\"  Action: {action.action_type}\")\n",
    "    print(f\"  Parameters: {action.parameters}\")\n",
    "    print(f\"  Score: {action.score:.3f} (基于 +/- logits 的概率)\")\n",
    "    print(f\"  Reasoning: {action.response[:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bd0a1d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 方法 3: Pairwise Judge (OpenAI GPT-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f3a9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置 Pairwise Judge (使用 GPT-5)\n",
    "config_pairwise = StepSelectionConfig(\n",
    "    enabled=True,\n",
    "    selection_method=\"pairwise_judge\",\n",
    "    num_candidates=3,\n",
    "    top_k=3,\n",
    "    pairwise_judge_type=\"openai\",\n",
    "    pairwise_judge_model=\"gpt-5\",\n",
    "    pairwise_sort_method=\"merge\"\n",
    ")\n",
    "\n",
    "# 创建 OpenAI Pairwise Judge Agent\n",
    "reward_agent_pairwise = OpenAIRewardAgent(config_pairwise)\n",
    "\n",
    "print(\"✓ Pairwise Judge Agent (GPT-5) 初始化完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b71f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建候选 actions\n",
    "candidates_pairwise = [\n",
    "    ActionCandidate(\n",
    "        action_type=\"move_object\",\n",
    "        parameters={\"object_id\": 2, \"target_position\": [0.1, 0.2, 0.3]},\n",
    "        response=\"I will move object 2 to position (0.1, 0.2, 0.3) to start building the structure.\",\n",
    "        tool_call={\"id\": \"call_1\", \"type\": \"function\", \"function\": {\"name\": \"move_object\", \"arguments\": '{\"object_id\": 2, \"target_position\": [0.1, 0.2, 0.3]}'}}\n",
    "    ),\n",
    "    ActionCandidate(\n",
    "        action_type=\"move_object\",\n",
    "        parameters={\"object_id\": 3, \"target_position\": [0.0, 0.0, 0.1]},\n",
    "        response=\"I will move object 3 to the base position to create a stable foundation.\",\n",
    "        tool_call={\"id\": \"call_2\", \"type\": \"function\", \"function\": {\"name\": \"move_object\", \"arguments\": '{\"object_id\": 3, \"target_position\": [0.0, 0.0, 0.1]}'}}\n",
    "    ),\n",
    "    ActionCandidate(\n",
    "        action_type=\"move_object\",\n",
    "        parameters={\"object_id\": 4, \"target_position\": [-0.1, 0.1, 0.2]},\n",
    "        response=\"I will place object 4 at this position to complete the first layer.\",\n",
    "        tool_call={\"id\": \"call_3\", \"type\": \"function\", \"function\": {\"name\": \"move_object\", \"arguments\": '{\"object_id\": 4, \"target_position\": [-0.1, 0.1, 0.2]}'}}\n",
    "    )\n",
    "]\n",
    "\n",
    "# 使用 pairwise judge 评分\n",
    "print(\"使用 Pairwise Judge (GPT-5) 方法评分...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "best_actions_pairwise = reward_agent_pairwise.select_best_actions(\n",
    "    candidates=candidates_pairwise,\n",
    "    current_image=current_image,\n",
    "    task_description=task_description,\n",
    "    object_mapping=object_mapping,\n",
    "    interaction_history=interaction_history\n",
    ")\n",
    "\n",
    "print(f\"\\nPairwise Judge 选择了 top-{len(best_actions_pairwise)} 个最佳 actions:\\n\")\n",
    "\n",
    "for i, action in enumerate(best_actions_pairwise, 1):\n",
    "    print(f\"第 {i} 名:\")\n",
    "    print(f\"  Action: {action.action_type}\")\n",
    "    print(f\"  Parameters: {action.parameters}\")\n",
    "    print(f\"  Score: {action.score:.3f} (相对得分)\")\n",
    "    print(f\"  Reasoning: {action.response[:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5578f3b",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "### 方法 1: Reward Model - Generative (Transformers 本地模型)\n",
    "- **模型**: ob11/Qwen-VL-PRM-3B (本地)\n",
    "- **模式**: Generative\n",
    "- **优点**: 独立评分，速度快（O(n) 次调用）\n",
    "- **评分范围**: 1-5 分（带详细推理）\n",
    "- **适用场景**: 候选较多时更高效，需要解释性评分\n",
    "\n",
    "### 方法 2: Reward Model - Discriminative (Transformers 本地模型)\n",
    "- **模型**: ob11/Qwen-VL-PRM-3B (本地)\n",
    "- **模式**: Discriminative\n",
    "- **优点**: 基于 logits 的概率评分，更快速\n",
    "- **评分方式**: 提取 +/- token 的 logits，softmax 后得到概率（转换为 1-5 分）\n",
    "- **适用场景**: 需要快速二元判断的场景，适合 PRM 模型\n",
    "\n",
    "### 方法 3: Pairwise Judge (OpenAI GPT-5)\n",
    "- **模型**: GPT-5 (API 调用)\n",
    "- **优点**: 相对比较，排序更准确\n",
    "- **评分方式**: 基于胜负次数的相对得分 (0-1)\n",
    "- **复杂度**: \n",
    "  - bubble: O(n²) 次比较\n",
    "  - merge: O(n log n) 次比较\n",
    "  - full: O(n²) 次比较（最准确）\n",
    "- **适用场景**: 需要精确排序，候选较少时使用\n",
    "\n",
    "### 推荐组合\n",
    "- **Reward Model - Generative**: 适合需要详细评分解释的场景\n",
    "- **Reward Model - Discriminative**: 适合快速评分，特别是使用 PRM (Process Reward Model)\n",
    "- **Pairwise Judge**: OpenAI GPT-5 用于最准确的排序（成本较高）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cae4cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 完成示例\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"三种方法对比完成！\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n对比总结:\")\n",
    "print(\"1. Generative 模式: 提供详细的 1-5 分评分和推理\")\n",
    "print(\"2. Discriminative 模式: 基于 +/- token logits 的快速评分\")\n",
    "print(\"3. Pairwise Judge: 通过成对比较获得最准确的排序\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
